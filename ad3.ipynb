{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0010497",
   "metadata": {},
   "source": [
    "# AD3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d585c0",
   "metadata": {},
   "source": [
    "Esta es la actividad dirigida 3 que consiste en hacer un ejercicio de programación literaria aprovechando el código que hemos usado en programación con Python donde realizamos *web scraping*. \n",
    "A continuación pongo el código fuente.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09529f56",
   "metadata": {},
   "source": [
    "## Instalar librerías"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2cafe6",
   "metadata": {},
   "source": [
    "En estos casos no es necesario instalar aquellas librerías que vienen con *Python*, pero las `externas` sí deben ser ___instaladas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3273e364",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\fanny\\documents\\python scripts\\lib\\site-packages (2.27.1)\n",
      "Collecting bs4\n",
      "  Using cached bs4-0.0.1-py3-none-any.whl\n",
      "Requirement already satisfied: pandas in c:\\users\\fanny\\documents\\python scripts\\lib\\site-packages (1.4.2)\n",
      "Collecting termcolor\n",
      "  Using cached termcolor-1.1.0-py3-none-any.whl\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\fanny\\documents\\python scripts\\lib\\site-packages (from requests) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\fanny\\documents\\python scripts\\lib\\site-packages (from requests) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\fanny\\documents\\python scripts\\lib\\site-packages (from requests) (1.26.9)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\fanny\\documents\\python scripts\\lib\\site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\fanny\\documents\\python scripts\\lib\\site-packages (from bs4) (4.11.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\fanny\\documents\\python scripts\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\fanny\\documents\\python scripts\\lib\\site-packages (from pandas) (1.21.5)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\fanny\\documents\\python scripts\\lib\\site-packages (from pandas) (2021.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\fanny\\documents\\python scripts\\lib\\site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\fanny\\documents\\python scripts\\lib\\site-packages (from beautifulsoup4->bs4) (2.3.1)\n",
      "Installing collected packages: termcolor, bs4\n",
      "Successfully installed bs4-0.0.1 termcolor-1.1.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install requests bs4 pandas termcolor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f82aa99",
   "metadata": {},
   "source": [
    "# Importar librerías\n",
    "\n",
    "\n",
    "## Librerías y módulos\n",
    "\n",
    "Aquí voy a importar las siguientes módulos y librerías\n",
    "\n",
    "**Librerías externas**\n",
    "- [requests](https://requests.readthedocs.io/en/latest/): Es una librería capaz de facilitar y abstraer el proceso de hacer solicitudes HTTP en Python en gran manera.\n",
    "- [bs4](https://pypi.org/project/beautifulsoup4/): Beautiful Soup es una librería Python que permite extraer información de contenido en formato HTML o XML.\n",
    "- [pandas](https://pypi.org/project/pandas/): Es una librería de Python especializada en el manejo y análisis de estructuras de datos.\n",
    "- [termcolor](https://replit.com/talk/learn/How-to-Use-Termcolor-In-Python/24684): \n",
    "\n",
    "**Módulos internos del sistema** \n",
    "- [time](https://docs.python.org/es/3/library/time.html): El módulo time de la biblioteca estándar de Python proporciona un conjunto de funciones para trabajar con fechas y/o horas. \n",
    "- [csv](https://docs.python.org/es/3/library/csv.html): El módulo csv implementa clases para leer y escribir datos tabulares en formato CSV (valores separados por comas).\n",
    "- [re](https://docs.python.org/es/3/library/re.html): Las funciones de este módulo permiten comprobar si una determinada cadena coincide con una expresión regular dada. \n",
    "- [os](https://docs.python.org/es/3.10/library/os.html):Este módulo permite a usted realizar operaciones dependiente del Sistema Operativo como crear una carpeta, listar contenidos de una carpeta, conocer acerca de un proceso y finalizar un proceso. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "111e1f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import csv\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import pandas as pd\n",
    "from termcolor import colored"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a132a175",
   "metadata": {},
   "source": [
    "## Código fuente de la actividad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bcf8444",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import csv\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import pandas as pd\n",
    "from termcolor import colored\n",
    " \n",
    "resultados = []\n",
    " \n",
    "req = requests.get(\"https://resultados.elpais.com\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup = BeautifulSoup(req.text, 'html.parser')\n",
    " \n",
    "tags = soup.findAll(\"h2\")\n",
    " \n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    " \n",
    "req2 = requests.get(\"https://elpais.com/internacional\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup2 = BeautifulSoup(req2.text, 'html.parser')\n",
    " \n",
    "tags = soup2.findAll(\"h2\")\n",
    " \n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    " \n",
    "req3 = requests.get(\"https://elpais.com/opinion\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup3 = BeautifulSoup(req3.text, 'html.parser')\n",
    " \n",
    "tags = soup3.findAll(\"h2\")\n",
    " \n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    " \n",
    "req4 = requests.get(\"https://elpais.com/espana/\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup4 = BeautifulSoup(req4.text, 'html.parser')\n",
    " \n",
    "tags = soup4.findAll(\"h2\")\n",
    " \n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    " \n",
    "req5 = requests.get(\"https://elpais.com/economia/\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup5 = BeautifulSoup(req5.text, 'html.parser')\n",
    " \n",
    "tags = soup5.findAll(\"h2\")\n",
    " \n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    " \n",
    "req6 = requests.get(\"https://elpais.com/sociedad/\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup6 = BeautifulSoup(req6.text, 'html.parser')\n",
    " \n",
    "tags = soup6.findAll(\"h2\")\n",
    " \n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    " \n",
    "req7 = requests.get(\"https://elpais.com/educacion/\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup7 = BeautifulSoup(req7.text, 'html.parser')\n",
    " \n",
    "tags = soup7.findAll(\"h2\")\n",
    " \n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    " \n",
    "req8 = requests.get(\"https://elpais.com/clima-y-medio-ambiente/\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup8 = BeautifulSoup(req8.text, 'html.parser')\n",
    " \n",
    "tags = soup8.findAll(\"h2\")\n",
    " \n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    " \n",
    "req9 = requests.get(\"https://elpais.com/ciencia/\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup9 = BeautifulSoup(req9.text, 'html.parser')\n",
    " \n",
    "tags = soup9.findAll(\"h2\")\n",
    " \n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    " \n",
    "req10 = requests.get(\"https://elpais.com/cultura/\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup10 = BeautifulSoup(req10.text, 'html.parser')\n",
    " \n",
    "tags = soup10.findAll(\"h2\")\n",
    " \n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    " \n",
    "req11 = requests.get(\"https://elpais.com/babelia/\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup11 = BeautifulSoup(req11.text, 'html.parser')\n",
    " \n",
    "tags = soup11.findAll(\"h2\")\n",
    " \n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    " \n",
    "req12 = requests.get(\"https://elpais.com/deportes/\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup12 = BeautifulSoup(req12.text, 'html.parser')\n",
    " \n",
    "tags = soup12.findAll(\"h2\")\n",
    " \n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    " \n",
    "req13 = requests.get(\"https://elpais.com/tecnologia/\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup13 = BeautifulSoup(req13.text, 'html.parser')\n",
    " \n",
    "tags = soup13.findAll(\"h2\")\n",
    " \n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    " \n",
    "req14 = requests.get(\"https://elpais.com/tecnologia/\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup14 = BeautifulSoup(req14.text, 'html.parser')\n",
    " \n",
    "tags = soup14.findAll(\"h2\")\n",
    " \n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    " \n",
    "req15 = requests.get(\"https://elpais.com/gente/\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup15 = BeautifulSoup(req15.text, 'html.parser')\n",
    " \n",
    "tags = soup15.findAll(\"h2\")\n",
    " \n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    " \n",
    "req16 = requests.get(\"https://elpais.com/television/\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup16 = BeautifulSoup(req16.text, 'html.parser')\n",
    " \n",
    "tags = soup16.findAll(\"h2\")\n",
    " \n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    " \n",
    "req17 = requests.get(\"https://elpais.com/eps/\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup17 = BeautifulSoup(req17.text, 'html.parser')\n",
    " \n",
    "tags = soup17.findAll(\"h2\")\n",
    " \n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    " \n",
    " \n",
    "os.system(\"clear\")\n",
    " \n",
    "print(colored(\"A continuación se muestran los titulares de las páginas principales del diario El País que contienen las siguientes palabras:\", 'blue', attrs=['bold']))\n",
    "print(colored(\"Feminismo\", 'green', attrs=['bold']))\n",
    " \n",
    "str_match = [s for s in resultados if \"feminismo\" in s]\n",
    "print(\"\\n\".join(str_match))\n",
    " \n",
    "print(colored(\"Igualdad\", 'green', attrs=['bold']))\n",
    " \n",
    "str_match = [s for s in resultados if \"igualdad\" in s]\n",
    "print(\"\\n\".join(str_match))\n",
    " \n",
    "print(colored(\"Mujeres\", 'green', attrs=['bold']))\n",
    " \n",
    "str_match = [s for s in resultados if \"mujeres\" in s]\n",
    "print(\"\\n\".join(str_match))\n",
    " \n",
    "print(colored(\"Mujer\", 'green', attrs=['bold']))\n",
    " \n",
    "str_match = [s for s in resultados if \"mujer\" in s]\n",
    "print(\"\\n\".join(str_match))\n",
    " \n",
    "print(colored(\"Brecha salarial\", 'green', attrs=['bold']))\n",
    " \n",
    "str_match = [s for s in resultados if \"brecha salarial\" in s]\n",
    "print(\"\\n\".join(str_match))\n",
    " \n",
    "print(colored(\"Machismo\", 'green', attrs=['bold']))\n",
    " \n",
    "str_match = [s for s in resultados if \"machismo\" in s]\n",
    "print(\"\\n\".join(str_match))\n",
    " \n",
    "print(colored(\"Violencia\", 'green', attrs=['bold']))\n",
    " \n",
    "str_match = [s for s in resultados if \"violencia\" in s]\n",
    "print(\"\\n\".join(str_match))\n",
    " \n",
    "print(colored(\"Maltrato\", 'green', attrs=['bold']))\n",
    " \n",
    "str_match = [s for s in resultados if \"maltrato\" in s]\n",
    "print(\"\\n\".join(str_match))\n",
    " \n",
    "print(colored(\"Homicidio\", 'green', attrs=['bold']))\n",
    " \n",
    "str_match = [s for s in resultados if \"homicidio\" in s]\n",
    "print(\"\\n\".join(str_match))\n",
    " \n",
    "print(colored(\"Género\", 'green', attrs=['bold']))\n",
    " \n",
    "str_match = [s for s in resultados if \"género\" in s]\n",
    "print(\"\\n\".join(str_match))\n",
    " \n",
    "print(colored(\"Asesinato\", 'green', attrs=['bold']))\n",
    " \n",
    "str_match = [s for s in resultados if \"asesinato\" in s]\n",
    "print(\"\\n\".join(str_match))\n",
    " \n",
    "print(colored(\"Sexo\", 'green', attrs=['bold']))\n",
    " \n",
    "str_match = [s for s in resultados if \"sexo\" in s]\n",
    "print(\"\\n\".join(str_match))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543f4d69",
   "metadata": {},
   "source": [
    "# Objetos/variables\n",
    "\n",
    "Se declara la **variable** resultados para almacenar cada resultado de las `URL`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91dea00",
   "metadata": {},
   "source": [
    "resultados = [] En esta celda se verán los resultados una vez se haya ejecutado el Web _Scrapping_. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "effed30e",
   "metadata": {},
   "source": [
    "# Nota\n",
    "Voy a empezar la actividad de **programación literaria**, algo nuevo para mí. \n",
    "\n",
    "Para llevar a cabo esta actividad seleccioné algunos de los **códigos** dados en el código principal, es decir tomaré algunas partes del código para ejecutar. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7fc99d3",
   "metadata": {},
   "source": [
    "Para empezar lo haré con la sección de **Educación**. Como ocurre es los ejercicios de Web *Scrapping*, hay que hacer una petición. \n",
    "La mencionada petición se hace con *req = requests.get(+URL)*. If es una condicional que *if (req.status_code != 200)*, que es el código de respuesta de estado satisfactorio o exitosa de la petición. Se pasa a extraer el texto HTML de la página web con  BeautifulSoup(req.text, 'html.parser').\n",
    "Entonces, cómo se hace el filtrado. Recurrimos a usar una de las funciones de soup, la findAll. Así solo se guardará en tags (que es una variable) el texto de los titulares del sitio web. No obstante, si se desean almacenar de forma individual sería en resultados, se usa el bucle for (*se utiliza para repetir una o más instrucciones un determinado número de veces*). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "797152e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primero de carrera: los bachilleres de la escuela pública son los que más materias aprueban \n",
      "Universidad en reconstrucción \n",
      "Volver a la escuela en América Latina tras dos años de pandemia\n",
      "El intento de suicidio de Juan y las somatizaciones de Carmen: ¿por qué los colegios necesitan un psicólogo educativo?\n",
      "El nuevo sistema para evaluar los conocimientos digitales de los profesores valdrá en toda España\n",
      "Un aula de Vallecas se rebela contra los libros de texto que silencian a las mujeres\n",
      "Solo un 3% de alumnos de Cataluña pidió hacer el examen de Selectividad en castellano\n",
      "Subirats reinterpreta la ley de Universidades: freno a la precariedad, facilidades para los alumnos extranjeros y ciencia abierta\n",
      "“Los profesores no van a cambiar de golpe su forma de trabajar el curso que viene por la nueva ley educativa”\n",
      "Trampantojo de pescado y lasaña vegetal en el comedor: cientos de colegios buscan fórmulas para que llevar la alimentación responsable a los niños\n",
      "Vídeo | Las recetas del cocinero escolar para que los niños de El Grau coman sano y rico \n",
      "Sin currículos\n",
      "La escuela concertada ante las desigualdades: el debate pendiente\n",
      "La equidad frente a las políticas educativas de privatización en Andalucía\n",
      "No hay lunes al sol en la Universidad\n",
      "Ofrecer comedor gratis en todos los colegios públicos es “alcanzable y urgente”: costaría 1.664 millones al año, según la ONG Educo  \n",
      "Una fórmula para que la escuela pública compita mejor con la concertada\n",
      "La pérdida de alumnos por el descenso de la natalidad está afectando con más fuerza a la escuela pública que a la concertada\n",
      "El Ayuntamiento de Madrid guarda en un cajón una herramienta ya pagada para ver los datos de contaminación al detalle \n",
      "La implantación del nuevo Bachillerato general fracasa pese a su demanda potencial: “Creímos que lo pedirían seis alumnos y lo han hecho 27”\n",
      "Toni Solano, director de instituto: “Veo mal a los niños, necesitan muchísima ayuda”\n",
      "Niños, Administraciones y redes sociales: prohibir su uso con una mano y enseñar a crear contenidos con la otra\n",
      "El Gobierno aprueba el decreto de bachillerato: los alumnos podrán terminar con un suspenso y desembocará en una nueva Selectividad\n",
      "La ley del silencio dentro y fuera del aula\n",
      "Las universitarias desertan del grado de Matemáticas ahora que tiene pleno empleo. ¿Por qué?\n",
      "Golpe a la temporalidad en las universidades: 25.000 profesores asociados serán indefinidos a tiempo parcial\n",
      "Antonio Abril: “Yo le decía a Castells: ‘Tienes que aguantar la presión, tienes que hacer la reforma universitaria”\n",
      "Los universitarios extranjeros podrán quedarse un año en España automáticamente al terminar la carrera\n"
     ]
    }
   ],
   "source": [
    "req7 = requests.get(\"https://elpais.com/educacion/\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup7 = BeautifulSoup(req7.text, 'html.parser')\n",
    " \n",
    "tags = soup7.findAll(\"h2\")\n",
    " \n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dab2826",
   "metadata": {},
   "source": [
    "# Otro código\n",
    "\n",
    "En siguiente código se repite el **proceso** realizado en el anterior, lo único que cambia es la sección en este caso es [Deportes](https://elpais.com/deportes/): *Un req12 = requests.get* para hacer el web scrapping; *if (req.status_code != 200)* para comprobar la petición; *soup = BeautifulSoup(req.text, 'html.parser')* para comprobar que ha sido exitosa; y *tags = soup.findAll(\"h2\")* para almacenar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb3fb35d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fernando Carro, del cielo al infierno, y regreso\n",
      "Bagnaia aprovecha el lío entre Quartararo y Espargaró para ganar el Gran Premio de los Países Bajos de Moto GP  \n",
      "GP de Assen: 30 años de la histórica victoria de Àlex Crivillé\n",
      "El doble regalo del deporte a Girona\n",
      "Aquel homenaje a Zico\n",
      "El silencio en los fichajes\n",
      "El ‘Cuerdo’ Bielsa\n",
      "Las criptomonedas saltan al deporte: una historia de conveniencia y desconfianza\n",
      "Tres años después, Nadal contra la amnesia\n",
      "Los días de vino y rosas del ‘calciomercato’\n",
      "Álex Márquez ficha por el Ducati Gresini\n",
      "Torneo de Candidatos de ajedrez de Madrid, en directo\n",
      "Pioneros para siempre\n",
      "Gareth Bale se va a EE UU a jugar en el equipo de Magic Johnson\n",
      "Eugenio López Chacarra, del golf amateur a millonario fichaje saudí\n",
      "Apoteosis de Italia, oro en el relevo de 4x100 estilos del Mundial de Natación\n",
      "La selección femenina de fútbol golea a Australia a dos semanas de la Eurocopa (7-0)\n",
      "Pecco Bagnaia se lleva la pole en el GP de los Países Bajos\n",
      "Nadal: “Confío en llegar competitivo”\n",
      "Niepómniashi y Caruana se destacan tras la primera vuelta\n",
      "La nueva gran atracción del fútbol español\n",
      "“Nunca acepto un no por respuesta”\n",
      "La mayor fábrica de baloncesto de Europa\n",
      "Crónica de dos ciudades moldeadas por la misma pasión \n",
      "La FINA prohíbe a Anita Álvarez, la nadadora que se desmayó, participar en la final por equipos en los Mundiales\n",
      "El circuito europeo de golf multa con 116.000 euros a los fugados a la liga saudí\n",
      "Wimbledon no permite escapatoria: duro trazado para Nadal y Alcaraz\n",
      "Baloncesto para derribar fronteras: de un campo de refugiados palestino a jugar contra el Estudiantes\n",
      "El silencio en los fichajes\n",
      "Elecciones en el Athletic de Bilbao: Barkala, Arechabaleta y Uriarte, tres aspirantes al trono de Ibaigane\n",
      "Jugar al ajedrez con 100 años: “Hay que ejercitar el cerebro como el resto del cuerpo”\n",
      "Orlando Magic da la sorpresa al elegir a Paolo Banchero en el primer puesto del Draft de la NBA\n"
     ]
    }
   ],
   "source": [
    "req12 = requests.get(\"https://elpais.com/deportes/\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup12 = BeautifulSoup(req12.text, 'html.parser')\n",
    " \n",
    "tags = soup12.findAll(\"h2\")\n",
    " \n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a31a66",
   "metadata": {},
   "source": [
    "# Nota\n",
    "He hecho la **descripción** de estos dos códigos, ya que la **solicitud se repite** en el resto de las secciones, que en total son 17. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3cf514c",
   "metadata": {},
   "source": [
    "# Código de color\n",
    "Ahora vez se muestran las **categorías** por las que se van a clasificar los titulares que se han imprimido. \n",
    "Primero, se verán por consola las **categorías** por las que se van a clasificar/segmentar los titulares extraídos; para filtrar los titulares por categoría se una un *bucle* for; y para finalizar, se verán todos los titulares de la **categoría en concreto**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41847a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(colored(\"Igualdad\", 'green', attrs=['bold']))\n",
    "\n",
    "str_match = [s for s in resultados if \"igualdad\" in s]\n",
    "print(\"\\n\".join(str_match))\n",
    "\n",
    "print(colored(\"Mujeres\", 'green', attrs=['bold']))\n",
    "\n",
    "str_match = [s for s in resultados if \"mujeres\" in s]\n",
    "print(\"\\n\".join(str_match))\n",
    "\n",
    "print(colored(\"Mujer\", 'green', attrs=['bold']))\n",
    "\n",
    "str_match = [s for s in resultados if \"mujer\" in s]\n",
    "print(\"\\n\".join(str_match))\n",
    "\n",
    "print(colored(\"Brecha salarial\", 'green', attrs=['bold']))\n",
    "\n",
    "str_match = [s for s in resultados if \"brecha salarial\" in s]\n",
    "print(\"\\n\".join(str_match))\n",
    "\n",
    "print(colored(\"Machismo\", 'green', attrs=['bold']))\n",
    "\n",
    "str_match = [s for s in resultados if \"machismo\" in s]\n",
    "print(\"\\n\".join(str_match))\n",
    "\n",
    "print(colored(\"Violencia\", 'green', attrs=['bold']))\n",
    "\n",
    "str_match = [s for s in resultados if \"violencia\" in s]\n",
    "print(\"\\n\".join(str_match))\n",
    "\n",
    "print(colored(\"Maltrato\", 'green', attrs=['bold']))\n",
    "\n",
    "str_match = [s for s in resultados if \"maltrato\" in s]\n",
    "print(\"\\n\".join(str_match))\n",
    "\n",
    "print(colored(\"Homicidio\", 'green', attrs=['bold']))\n",
    "\n",
    "str_match = [s for s in resultados if \"homicidio\" in s]\n",
    "print(\"\\n\".join(str_match))\n",
    "\n",
    "print(colored(\"Género\", 'green', attrs=['bold']))\n",
    "\n",
    "str_match = [s for s in resultados if \"género\" in s]\n",
    "print(\"\\n\".join(str_match))\n",
    "\n",
    "print(colored(\"Asesinato\", 'green', attrs=['bold']))\n",
    "\n",
    "str_match = [s for s in resultados if \"asesinato\" in s]\n",
    "print(\"\\n\".join(str_match))\n",
    "\n",
    "print(colored(\"Sexo\", 'green', attrs=['bold']))\n",
    "\n",
    "str_match = [s for s in resultados if \"sexo\" in s]\n",
    "print(\"\\n\".join(str_match))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caea0c04",
   "metadata": {},
   "source": [
    "# Comentarios\n",
    "\n",
    "Tras haber realizado la actividad, quería hacer algunas anotaciones: \n",
    "1. Fue muy **interesante** tratar de descrifrar cada uno de los pasos de los diferentes códigos. \n",
    "2. Una **asignación** compleja, pero de la que se aprende algo nuevo. \n",
    "3. Al final el print(colored no me imprimió, pero logré hacer la explicación. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
